{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {},
   "source": [
    "# Hogwild! Parallelism: Basic Example\n",
    "\n",
    "This example demonstrates Hogwild! inference on a single problem with 2 workers and minimal prompt defined below. There are no few-shot examples or prompt insertions, and the cache layout is the simplest one possible: two contiguous workspaces. This notebook is intended as a playground while the other notebooks present more advanced prompting and cache layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import shared_cache\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "MODEL_NAME = \"Qwen/QwQ-32B\"  # for 48GB gpus, use \"Qwen/QwQ-32B-AWQ\" instead\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)\n",
    "\n",
    "parallelism_prompt_common = \"\"\"\n",
    "I will collaborate this problem with another. We refer to each other as Alice and Bob. We are assistants.\n",
    "\n",
    "We will reason together and try to collaborate. I will take into account what the other assistant is doing and try to help them.\n",
    "\n",
    "We will write our solutions concurrently. I will write my own thoughts at the bottom, and see the other's thoughts above.\n",
    "\n",
    "I will not repeat the copy assistant's thoughts: I can already see them above.\n",
    "\n",
    "The other assistant will continue writing their thoughts above while I am writing mine. They will add more text every time I check.\n",
    "\n",
    "Since we both write our thoughts in parallel, I will initially see only partial (unfinished) thoughts of the other assistant.\n",
    "I will use these partial thoughts to decide how best to help the other assistant without doing the same work twice.\n",
    "\n",
    "When reasoning, we will give each other tasks to coordinate (e.g. if Alice writes: Bob, please do this, then Bob should take this into account).\n",
    "\n",
    "Before doing anything, I will check the other assistant's workspace. If they have already done that or are currently doing it, I don't need to do that again. If so, I will stop (e.g. 'Wait, this is already done') and pivot to a different task.\n",
    "\"\"\".strip()\n",
    "\n",
    "worker_headers = [\"\\n\\n# Alice workspace\\n\\n\", \"\\n\\n# Bob workspace\\n\\n\"]\n",
    "prompt_split = \" <the assistant will continue here>\\n\\n\"\n",
    "\n",
    "forbidden_token_ix = [tokenizer.vocab[x] for x in (\"#\", \"</think>\")]\n",
    "for x in tokenizer.special_tokens_map.values():\n",
    "    forbidden_token_ix.extend([tokenizer.vocab[x]] if isinstance(x, str) else map(tokenizer.vocab.get, x))\n",
    "tokenizer_kwargs = dict(add_special_tokens=False, return_tensors='pt', padding=True, padding_side='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354601ba",
   "metadata": {},
   "source": [
    "__Playground:__ you can define a problem and see if the workers collaborate. With this simple setup, they do not always do that well out of the box, but this allows you to see how the prompt impacts their actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa1ebd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "problem = \"\"\"Calculate x - x^2 + x^3 for x = 5,6,7,8. Alice must return all 4 answers in \\boxed{ }.\"\"\"\n",
    "\n",
    "prompt_full_input = tokenizer.apply_chat_template(\n",
    "    [dict(role='user', content=problem)], tokenize=False, add_generation_prompt=True\n",
    ") + \"\\n\\n\" + parallelism_prompt_common\n",
    "\n",
    "worker_prompts = [\n",
    "    f\"\"\"{worker_headers[0]}I am Alice. Let's solve this together, Bob. Here's how we should collaborate:\"\"\",\n",
    "    f\"\"\"{worker_headers[1]}I am Bob. Let's solve this together, Alice.\"\"\"\n",
    "]\n",
    "\n",
    "cache_input, cache_split, cache_w1, cache_w2 = (shared_cache.CacheBlock(config=model.config) for _ in range(4))\n",
    "cm = shared_cache.SharedCacheManager(cache_structure=[\n",
    "    [cache_input, cache_w2, cache_split, cache_w1],\n",
    "    [cache_input, cache_w1, cache_split, cache_w2],\n",
    "], write_to=[cache_w1, cache_w2])\n",
    "\n",
    "# pre-fill common parts\n",
    "with torch.inference_mode():\n",
    "    model(**tokenizer(prompt_full_input, **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_input);  # <-- write to common prompt\n",
    "    model(**tokenizer(prompt_split, **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_split);   # <-- write to common separator\n",
    "\n",
    "# generate tokens in parallel with each worker\n",
    "next_inputs = tokenizer(worker_prompts, **tokenizer_kwargs).to(device)\n",
    "tokens_by_worker = tokenizer(worker_prompts, add_special_tokens=False)[\"input_ids\"]\n",
    "for inference_step in range(1024):       # <-- change max tokens here\n",
    "    with torch.inference_mode():\n",
    "        logits = model(**cm.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "        logits[..., forbidden_token_ix] -= 100\n",
    "        new_tokens = logits.argmax(-1)   # <-- greedy generation\n",
    "        next_inputs = dict(input_ids=new_tokens.view(-1, 1))\n",
    "    \n",
    "    for worker_tokens, new_token in zip(tokens_by_worker, new_tokens.tolist()):\n",
    "        worker_tokens.append(new_token)\n",
    "    clear_output(True)\n",
    "    display(Markdown(\"\".join(tokenizer.decode(seq) for seq in tokens_by_worker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145adcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
