{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16\n",
    "import sys; sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm, trange\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import shared_cache\n",
    "\n",
    "MODEL_NAME = \"Qwen/QwQ-32B\" #\"Qwen/QwQ-32B-AWQ\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {},
   "source": [
    "# Hogwild parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa233814",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"\n",
    "Calculate x + x^2 + x^3 + x^4 + x^5 for x = 1..10\n",
    "\"\"\".strip()\n",
    "\n",
    "parallelism_prompt_common = \"\"\"\n",
    "I will collaborate this problem with another. We refer to each other as Alice and Bob. We are assistants.\n",
    "\n",
    "We will reason together and try to collaborate.\n",
    "I will take into account what the other assistant is doing and try to help them.\n",
    "\n",
    "We will write our solutions concurrently. I will write my own thoughts at the bottom, and see the other's thoughts above.\n",
    "\n",
    "I will not repeat the copy assistant's thoughts: I can already see them above.\n",
    "\n",
    "The other assistant will continue writing their thoughts above while I am writing mine. They will add more text every time I check.\n",
    "\n",
    "Since we both write our thoughts in parallel, I will initially see only partial (unfinished) thoughts of the other assistant.\n",
    "I will use these partial thoughts to decide how best to help the other assistant without doing the same work twice.\n",
    "\n",
    "When reasoning, we will five each other tasks to coordinate (e.g. if Alice writes: Bob, please do this, then Bob should take this into account).\n",
    "\n",
    "Before doing anything, I will check the other assistant's workspace. If they have already done that or are currently doing it, I don't need to do that again. If so, I will stop (e.g. 'Wait, this is already done') and pivot to a different task.\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "SEP = '\\n\\n'\n",
    "WORKER_PREFIXES = [SEP + \"# Alice workspace\" + SEP, SEP + \"# Bob workspace\" + SEP]\n",
    "\n",
    "prompt_full_input = tokenizer.apply_chat_template(\n",
    "    [dict(role='user', content=problem)],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ") + SEP + parallelism_prompt_common\n",
    "prompt_split = \" <the assistant will continue here>\\n\\n\"\n",
    "\n",
    "worker_prompts = [\n",
    "    WORKER_PREFIXES[0] + \"\"\"I am Alice. Let's solve this together, Bob. Here's how we should collaborate:\"\"\".strip(),\n",
    "    WORKER_PREFIXES[1] + \"\"\"I am Bob. Let's solve this together, Alice.\"\"\".strip()\n",
    "]\n",
    "\n",
    "forbidden_token_ix = [tokenizer.vocab[x] for x in ['#', '<|im_end|>', '</think>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa1ebd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cache_input, cache_split, cache_w1, cache_w2 = (shared_cache.CacheBlock(config=model.config) for _ in range(4))\n",
    "cm = shared_cache.SharedCacheManager(cache_structure=[\n",
    "    [cache_input, cache_w2, cache_split, cache_w1],\n",
    "    [cache_input, cache_w1, cache_split, cache_w2],\n",
    "], write_to=[cache_w1, cache_w2])\n",
    "\n",
    "# pre-fill common parts\n",
    "with torch.no_grad():\n",
    "    model(**tokenizer(prompt_full_input, add_special_tokens=False, return_tensors='pt').to(device),\n",
    "          use_cache=True, past_key_values=cache_input);  # <-- write to common prompt\n",
    "    model(**tokenizer(prompt_split, add_special_tokens=False, return_tensors='pt').to(device),\n",
    "          use_cache=True, past_key_values=cache_split);   # <-- write to common separator\n",
    "\n",
    "# generate texts\n",
    "next_inputs = tokenizer(worker_prompts, return_tensors='pt', padding=True, padding_side='left').to(device)\n",
    "tokens_by_worker = tokenizer(worker_prompts)['input_ids']\n",
    "for inference_step in range(1024):\n",
    "    with torch.no_grad():\n",
    "        logits = model(**cm.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "        logits[..., forbidden_token_ix] -= 100\n",
    "        new_tokens = logits.argmax(-1)\n",
    "        assert len(new_tokens) == len(cm.cache_structure)\n",
    "        next_inputs = dict(input_ids=new_tokens.view(-1, 1))\n",
    "\n",
    "    for worker_tokens, new_token in zip(tokens_by_worker, new_tokens.tolist()):\n",
    "        worker_tokens.append(new_token)\n",
    "    clear_output(True)\n",
    "    for worker_index, worker_tokens in enumerate(tokens_by_worker):\n",
    "        print(end=tokenizer.decode(worker_tokens))\n",
    "    print(flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
